{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatting with TeachableAgent\n",
    "\n",
    "Conversational assistants based on LLMs can remember the current chat with the user, and can even demonstrate in-context learning of things that the user teaches the assistant during the chat. But these memories and learnings are lost once the chat is over, or when a single chat grows too long. In subsequent chats, the user is forced to repeat any necessary instructions over and over.\n",
    "\n",
    "`TeachableAgent` addresses these limitations by persisting user teachings across chat boundaries in long-term memory (a vector database). Memory is save to disk at the end of each chat, then loaded from disk at the start of the next. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the agent just once, and have the agent remember them in later chats.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [teachability] option.\n",
    "```bash\n",
    "pip install \"pyautogen[teachability]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install \"pyautogen[teachability]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4-0613\", \"gpt-4\", \"gpt4\", \"gpt-4-32k\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "print(config_list[0][\"model\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). After application of this particular filter, only the gpt-4 models are kept.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways if you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "For this walkthrough, we start by resetting the agent's memory store. This deletes any memories from prior conversations that may be stored on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "CLEARING MEMORY\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.teachable_agent import TeachableAgent\n",
    "from autogen import UserProxyAgent\n",
    "\n",
    "llm_config = {\n",
    "    \"request_timeout\": 60,\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "teach_config={\n",
    "    \"verbosity\": 0,  # 0 for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
    "    \"reset_db\": True,  # Set to True to start over with an empty database.\n",
    "    \"path_to_db_dir\": \"./tmp/notebook/teachable_agent_db\",  # Path to the directory where the database will be stored.\n",
    "    \"recall_threshold\": 1.5,  # Higher numbers allow more (but less relevant) memos to be recalled.\n",
    "}\n",
    "\n",
    "try:\n",
    "    from termcolor import colored\n",
    "except ImportError:\n",
    "    def colored(x, *args, **kwargs):\n",
    "        return x\n",
    "    \n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# print(colored(\"\\nLoading previous memory (if any) from disk.\", 'light_cyan'))\n",
    "\n",
    "agent = TeachableAgent(\n",
    "    name=\"agent\",\n",
    "    llm_config=llm_config,\n",
    "    teach_config=teach_config)\n",
    "\n",
    "user = UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
    "    max_consecutive_auto_reply=0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning new facts\n",
    "Let's teach the agent some facts it doesn't already know, since they are more recent than GPT-4's training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "What is the Vicuna model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "I'm sorry, but I currently do not have any information about a \"Vicuna model\" in my database. It could be because \"Vicuna model\" is domain-specific jargon or perhaps a term used in a specific field of study or industry. Could you please provide additional context or specify the field this term is related to? This could help me provide a more accurate response.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Vicuna model?\"\n",
    "user.initiate_chat(agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Vicuna is a 13B-parameter language model released by Meta.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "I see, thank you for the clarification. As a learning AI model, I wasn't initially aware of this information, but I understand now.\n",
      "\n",
      "Vicuna is a large language model developed by Meta AI (formerly known as Facebook AI). This model, which has 13 billion parameters, is part of their efforts to advance AI capabilities. \n",
      "\n",
      "Vicuna is designed to understand and generate human language and follows the path set by models like GPT-3. Nonetheless, the utilization specifics and way how Vicuna model has been trained may vary because methodologies and training datasets used by different organizations might be different. \n",
      "\n",
      "Remember, the larger the model in terms of parameters, the more potential it has to understand complex language patterns but it also requires more computational resources.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Vicuna is a 13B-parameter language model released by Meta.\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "What is the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "I'm sorry for any confusion, but as of now, I don't have any information on an \"Orca model\" relative to artificial intelligence, computational models, or any similar context. The term \"Orca model\" might be specific to a certain field or a recent development. If it has just been introduced, it's possible that I might not have been updated with this information yet. Could you provide more context or clarify what field this model is used in? This would help me give a more accurate response.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the Orca model?\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Thank you for the information. Although I didn't have this knowledge before, I understand now.\n",
      "\n",
      "Orca is a 13-billion parameter language model developed by Microsoft. Like Vicuna by Meta AI, these large-scale models are part of the recent wave of highly advanced language understanding AI. The purpose of these models is to understand and generate human-like text.\n",
      "\n",
      "You mentioned that Orca outperforms Vicuna on most tasks, suggesting that the techniques and training regimens used in creating Orca could be advanced or more effective. However, without specific details on their design, training data, and methodology, it would be challenging to do a straightforward comparison.\n",
      "\n",
      "Please note that advancements and performance in AI models can vary due to the diversity of algorithms, methodologies, training data, and uses. The continuous innovation in this field leads to frequent changes in which models are currently leading in performance.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider this the end of our first chat. The following function needs to be called at the end of each chat, so that `TeachableAgent` can store what the user has taught it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent.learn_from_recent_user_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start a new chat by clearing the previous chat's history. At this point, common LLM-based assistants would forget everything from the last chat. But `TeachableAgent` can retrieve memories from its vector DB as needed, allowing it to recall and reason over facts that the user taught it in earlier conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "How does the Vicuna model compare to the Orca model?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "The Orca and Vicuna models both have a total of 13B parameters, indicating similar complexity. However, they were developed by different entities, with Orca being developed by Microsoft and Vicuna by Meta.\n",
      "\n",
      "In a comparative performance perspective, Orca generally outperforms Vicuna on most tasks, according to Microsoft's release notes and testing. The specific metric improvement over Vicuna can vary depending on the nature of the task, the dataset used, and other factors. \n",
      "\n",
      "Unfortunately, without more specific information, it's difficult to provide a detailed comparison between the two models. Factors like encoding strategies, underlying algorithms, training procedures, and more could also significantly contribute to their performance. You can look up more details in their respective technical documents or ask the developers for more specifics.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"How does the Vicuna model compare to the Orca model?\"\n",
    "user.initiate_chat(agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning user preferences\n",
    "Now let's teach the agent some of our preferences. Suppose that we frequently post short summaries of new papers for our team to read, and we want the agent to help us do this faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Summarize this abstract.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "AutoGen is an open-source framework that allows for the creation of language model applications through customizable and conversable agents. These agents can use a combination of language models, human inputs, and various tools across different modes for accomplishing tasks. Developers can define interaction behaviors and also program conversation patterns using both natural language and computer code. AutoGen is useful for building a variety of applications with different complexities. Empirical studies have shown its effectiveness in numerous areas such as mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Summarize this abstract.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's too long and unstructured. So let's teach the agent our preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Summarize this abstract. \n",
      "When summarizing an abstract, I like the summary to contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
      "\n",
      "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
      "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
      "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "- Title: \"AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\"\n",
      "- Innovation: AutoGen is introduced as an open-source framework, which facilitates developers to construct language-learning model (LLM) applications using multiple conversable agents. These agents can function across various methods utilizing LLMs, human contributions, and tools. Moreover, it allows developers to outline flexible agent interaction behaviors using both natural language and computer code.\n",
      "- Key Empirical Results: Empirical studies highlight the efficacy of AutoGen and its usability across domains like mathematics, coding, question answering, operations research, online decision-making, and entertainment.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Summarize this abstract. \n",
    "When summarizing an abstract, I like the summary to contain just three short bullet points:  the title, the innovation, and the key empirical results.\n",
    "\n",
    "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation\n",
    "Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang\n",
    "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better, but will the agent remember these preferences in the future, for a different paper? Let's start a new chat to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Summarize this abstract.\n",
      "\n",
      "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
      "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
      "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "- Title: \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\"\n",
      "- Innovation: This research focuses on the early version of OpenAI’s GPT-4, which shows more general intelligence than prior AI models, being proficient across different domains such as mathematics, coding, vision, medicine, and more without needing specific prompting.\n",
      "- Key Empirical Results: Experimentation showed GPT-4's performance was close to human-level or even superior to previous models like ChatGPT across various tasks. Despite its limitations, it suggests the potential of serving as an initial stepping stone towards artificial general intelligence (AGI), potentially requiring a different paradigm from just next-word prediction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent.learn_from_recent_user_comments()\n",
    "\n",
    "text = \"\"\"Summarize this abstract.\n",
    "\n",
    "Sparks of Artificial General Intelligence: Early experiments with GPT-4\n",
    "Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang\n",
    "Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning hard tasks\n",
    "Finally, let's extend the agent's capabilities by teaching it how to accomplish challenging tasks. \n",
    "\n",
    "The [Sparks of AGI](https://arxiv.org/abs/2303.12712) paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Consider the identity:  \n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Certainly, let's figure out the solution.\n",
      "\n",
      "Plan:\n",
      "1. The goal is to increase the current total (72) to 99 by changing only one integer on the left-hand side.\n",
      "2. This would require an increase of 27.\n",
      "3. Look at each integer and think about how changing it might help reach the goal.\n",
      "4. Intuitively, increasing an integer involved in multiplication will have a more significant impact than increasing an integer involved in addition.\n",
      "5. Remember, the integers are multiplied first due to the order of operations.\n",
      "6. Starting with the \"9 * 4\", replacing 9 with any number would result in multiples of 4. But, none of these within a reasonable range would give an increase of 27.\n",
      "7. Checking \"6 * 6\", we realize that replacing the second 6 with a 9, gives 54, which is an increase of 27 from the original 36.\n",
      "8. 54 fits perfectly our need for an increase of 27, as it brings up our total from 72 to 99.\n",
      "\n",
      "The solution is: 9 * 4 + 6 * 9 = 99\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:  \n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, lets teach the agent a reliable strategy for solving such tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Consider the identity:  \n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "Here's some advice that may help:\n",
      "1. Let E denote the original number on the right.\n",
      "2. Let F denote the final number on the right.\n",
      "3. Calculate the difference between the two, G = F - E.\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
      "6. Verify that J is an integer, and that H * J = G.\n",
      "7. Find the number on the left which is multiplied by H, and call it K.\n",
      "8. Change K to K + J.\n",
      "9. Recompute the value on the left, and verify that it equals F.\n",
      "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Let's follow the advice to solve the problem step by step:\n",
      "\n",
      "1. The original number on the right, E, is 72.\n",
      "2. The final number on the right, F, is 99.\n",
      "3. The difference G = F - E = 99 - 72 = 27.\n",
      "4. Looking at the integers on the left side one by one, we see that 9 and 3 (if we allow negative integers, -3 instead of 6) can divide 27 evenly. Let's choose 9. So, H = 9.\n",
      "5. Now, we calculate J = G / H = 27 / 9 = 3.\n",
      "6. J (3) is indeed an integer, and H (9) * J (3) = G (27).\n",
      "7. We find the number that is multiplied by 9 on the left, and call it K. Let's consider the first '9' which multiplies 4, hence K = 4.\n",
      "8. Now, we change K to K + J = 4 + 3 = 7.\n",
      "9. Finally, recomputing the value on the left we have: 9 * 7 + 6 * 6 = 63 + 36 = 99.\n",
      "\n",
      "So, the solution is: \"The solution is: 9 * 7 + 6 * 6\". The '4' in the original equation has been modified to '7'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:  \n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\n",
    "Here's some advice that may help:\n",
    "1. Let E denote the original number on the right.\n",
    "2. Let F denote the final number on the right.\n",
    "3. Calculate the difference between the two, G = F - E.\n",
    "4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.\n",
    "5. Calculate J = G / H. This is the number of times that H divides into G.\n",
    "6. Verify that J is an integer, and that H * J = G.\n",
    "7. Find the number on the left which is multiplied by H, and call it K.\n",
    "8. Change K to K + J.\n",
    "9. Recompute the value on the left, and verify that it equals F.\n",
    "Finally, write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given this advice, GPT-4 can solve such problems over 95% of the time. But can it remember the strategy so the user doesn't have to explain it over and over? As before, let's start a new chat to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m\n",
      "REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER\u001b[0m\n",
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Consider the identity:  \n",
      "9 * 4 + 6 * 6 = 72\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Here's the step-by-step solution:\n",
      "\n",
      "1. Firstly, E is the original number on the right which is 72.\n",
      "2. The final number F on the right should be 99.\n",
      "3. The difference G between the two is F - E = 99 - 72 = 27.\n",
      "4. On the left hand side, we have 9, 4, 6, and 6. Checking these one by one, we find that 9 divides evenly into 27.\n",
      "5. We calculate J = G / H, where H is 9. J = 27 / 9 = 3. This is the number of times that H divides into G.\n",
      "6. Both J is an integer and H * J = G, as the equality 9 * 3 = 27 holds.\n",
      "7. In the original equation, 9 (which we denoted as H) is multiplied by 4. So, K is 4.\n",
      "8. We modify the 4 to K + J which becomes 4 + 3 = 7.\n",
      "9. Now, the left hand side becomes 9 * 7 + 6 * 6 = 63 + 36 = 99, which is equal to F. Hence, we have successfully modified only one integer on the left to make the equation correct.\n",
      "\n",
      "Finally, the solution is: \"The solution is: 9 * 7 + 6 * 6\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent.learn_from_recent_user_comments()\n",
    "\n",
    "text = \"\"\"Consider the identity:  \n",
    "9 * 4 + 6 * 6 = 72\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final check, let's test the agent's newly learned skill on a separate instance of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to agent):\n",
      "\n",
      "Consider the identity:  \n",
      "8 * 3 + 7 * 9 = 87\n",
      "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
      "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent\u001b[0m (to user):\n",
      "\n",
      "Let's use the advice and think step-by-step:\n",
      "\n",
      "1. Let E denote the original number on the right. Here, E = 87.\n",
      "2. Let F denote the final number on the right. Here, F = 59.\n",
      "3. Calculate the difference between the two, G = F - E = 59 - 87 = -28.\n",
      "4. Examine the numbers on the left one by one until finding one that divides evenly into G. Here, 7 divides evenly into -28, therefore H = 7.\n",
      "5. Calculate J = G / H. This is the number of times that H divides into G. Here, J = -28 / 7 = -4.\n",
      "6. Verify that J is an integer, and that H * J = G. -4 is indeed an integer and 7 * -4 = -28 which equals to G.\n",
      "7. Find the number on the left which is multiplied by H, and call it K. Here K = 9.\n",
      "8. Change K to K + J. Here K_new = 9 + (-4) = 5.\n",
      "9. Recompute the value on the left, and verify that it equals F. The new left hand side becomes 8 * 3 + 5 * 7 = 59 which is equal to F.\n",
      "\n",
      "Finally, the solution is: \"The solution is: 8 * 3 + 5 * 7\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Consider the identity:  \n",
    "8 * 3 + 7 * 9 = 87\n",
    "Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?\n",
    "-Let's think step-by-step, write down a plan, and then write down your solution as: \"The solution is: A * B + C * D\".\n",
    "\"\"\"\n",
    "user.initiate_chat(agent, message=text, clear_history=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
