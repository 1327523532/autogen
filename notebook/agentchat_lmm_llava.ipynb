{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c75da30",
   "metadata": {},
   "source": [
    "# Agent Chat with Multi-Modality Models\n",
    "\n",
    "We use LLaVA as an example for the multi-modality feature. \n",
    "\n",
    "\n",
    "This notebook contains the following information and examples:\n",
    "\n",
    "1. Setup LLaVA \n",
    "    - Option 1: Use API calls from `Replicate`\n",
    "    - Option 2: Setup LLaVA locally (requires GPU)\n",
    "2. Application 1: Garden helper\n",
    "3. Application 2: Image Regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24a629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this variable to control where you want to host LLaVA, locally or remotely?\n",
    "# More details in the two setup options below.\n",
    "\n",
    "LLAVA_MODE = \"local\" # Either \"local\" or \"remote\"\n",
    "\n",
    "assert LLAVA_MODE in [\"local\", \"remote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f0bd0",
   "metadata": {},
   "source": [
    "# (Option 1, preferred) Use API Calls from Replicate [Remote]\n",
    "We can also use [Replicate](https://replicate.com/yorickvp/llava-13b/api) to use LLaVA directly, which will host the model for you.\n",
    "\n",
    "1. Run `pip install replicate` to install the package\n",
    "2. You need to get an API key from Replicate from your [account setting page](https://replicate.com/account/api-tokens)\n",
    "3. Next, copy your API token and authenticate by setting it as an environment variable:\n",
    "    `export REPLICATE_API_TOKEN=<paste-your-token-here>` \n",
    "4. You need to enter your credit card information for Replicate ðŸ¥²\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a707d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install replicate\n",
    "# import os\n",
    "## alternatively, you can put your API key here for the environment variable.\n",
    "# os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_xyz your api key goes here~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cd425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e313bc00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img = get_image_data(\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\", use_b64=True)\n",
    "# img = 'data:image/jpeg;base64,' + img\n",
    "\n",
    "# response = replicate.run(\n",
    "#     \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "#     input={\"image\": img,\n",
    "#            \"prompt\": \"Describe the in poetry.\"}\n",
    "# )\n",
    "# output = \"\"\n",
    "# for item in response:\n",
    "#     output += item\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805e4bd",
   "metadata": {},
   "source": [
    "## [Option 2] Setup LLaVA Locally\n",
    "Please follow the LLaVA GitHub [page](https://github.com/haotian-liu/LLaVA/) to install LLaVA, download the weights, and start the server.\n",
    "\n",
    "For instance, here are some important steps:\n",
    "```bash\n",
    "# Download the package\n",
    "git clone https://github.com/haotian-liu/LLaVA.git\n",
    "cd LLaVA\n",
    "\n",
    "# Install the inference package\n",
    "conda create -n llava python=3.10 -y\n",
    "conda activate llava\n",
    "pip install --upgrade pip  # enable PEP 660 support\n",
    "pip install -e .\n",
    "\n",
    "# Download and serve the model\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\n",
    "```\n",
    "\n",
    "Some helpful packages and dependencies:\n",
    "```bash\n",
    "conda install -c nvidia cuda-toolkit\n",
    "```\n",
    "\n",
    "\n",
    "### Launch\n",
    "\n",
    "In one terminal, start the controller first:\n",
    "```bash\n",
    "python -m llava.serve.controller --host 0.0.0.0 --port 10000\n",
    "```\n",
    "\n",
    "\n",
    "Then, in another terminal, start the worker, which will load the model to the GPU:\n",
    "```bash\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29925f",
   "metadata": {},
   "source": [
    "**Note: make sure the environment of this notebook also installed the llava package from `pip install -e .`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bf7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-17 16:11:27,800] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'models': ['llava-v1.5-13b']}\n",
      "Model Name: llava-v1.5-13b\n"
     ]
    }
   ],
   "source": [
    "# Run this code block only if you want to run LlaVA locally\n",
    "import requests\n",
    "import json\n",
    "from llava.conversation import default_conversation as conv\n",
    "from llava.conversation import Conversation\n",
    "\n",
    "# Setup some global constants for convenience\n",
    "# Note: make sure the addresses below are consistent with your setup in LLaVA \n",
    "WORKER_ADDR = \"http://0.0.0.0:40000\"\n",
    "CONTROLLER_ADDR = \"http://0.0.0.0:10000\"\n",
    "SEP =  conv.sep\n",
    "ret = requests.post(CONTROLLER_ADDR + \"/list_models\")\n",
    "print(ret.json())\n",
    "MODEL_NAME = ret.json()[\"models\"][0]\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d926a3",
   "metadata": {},
   "source": [
    "# Multi-Modal Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf7f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def extract_img_paths(paragraph: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract image paths (URLs or local paths) from a text paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The input text paragraph.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted image paths.\n",
    "    \"\"\"\n",
    "    # Regular expression to match image URLs and file paths\n",
    "    img_path_pattern = re.compile(r'\\b(?:http[s]?://\\S+\\.(?:jpg|jpeg|png|gif|bmp)|\\S+\\.(?:jpg|jpeg|png|gif|bmp))\\b', \n",
    "                                  re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the paragraph\n",
    "    img_paths = re.findall(img_path_pattern, paragraph)\n",
    "    return img_paths\n",
    "\n",
    "\n",
    "def get_image_data(image_file, use_b64=True):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        content = response.content\n",
    "    elif image_file.startswith(\"data:image/png;base64,\"):\n",
    "        return image_file.replace(\"data:image/png;base64,\", \"\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        content = open(image_file, \"rb\").read()\n",
    "        \n",
    "    if use_b64:\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "    \n",
    "def _to_pil(data):\n",
    "    return Image.open(BytesIO(base64.b64decode(data)))\n",
    "\n",
    "\n",
    "def llava_call(prompt:str, model_name: str=MODEL_NAME, images: list=[], max_new_tokens:int=1000) -> str:\n",
    "    \"\"\"\n",
    "    Makes a call to the LLaVA service to generate text based on a given prompt and optionally provided images.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): The input text for the model. Any image paths or placeholders in the text should be replaced with \"<image>\".\n",
    "        - model_name (str, optional): The name of the model to use for the text generation. Defaults to the global constant MODEL_NAME.\n",
    "        - images (list, optional): A list of image paths or URLs. If not provided, they will be extracted from the prompt.\n",
    "            If provided, they will be appended to the prompt with the \"<image>\" placeholder.\n",
    "        - max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        - str: Generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        - AssertionError: If the number of \"<image>\" tokens in the prompt and the number of provided images do not match.\n",
    "        - RunTimeError: If any of the provided images is empty.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses global constants: WORKER_ADDR and SEP.\n",
    "    - Any image paths or URLs in the prompt are automatically replaced with the \"<image>\" token.\n",
    "    - If more images are provided than there are \"<image>\" tokens in the prompt, the extra tokens are appended to the end of the prompt.\n",
    "    \"\"\"\n",
    "    if len(images) == 0:\n",
    "        images = extract_img_paths(prompt)\n",
    "        for im in images:\n",
    "            prompt = prompt.replace(im, \"<image>\")\n",
    "    else:\n",
    "        # Append the <image> token if missing\n",
    "        assert prompt.count(\"<image>\") <= len(images), \"the number \"\n",
    "        \"of image token in prompt and in the images list should be the same!\"\n",
    "        num_token_missing = len(images) - prompt.count(\"<image>\")\n",
    "        prompt += \" <image> \" * num_token_missing\n",
    "\n",
    "    \n",
    "    images = [get_image_data(x) for x in images]\n",
    "    \n",
    "    for im in images:\n",
    "        if len(im) == 0:\n",
    "            raise RunTimeError(\"An image is empty!\")\n",
    "            \n",
    "    headers = {\"User-Agent\": \"LLaVA Client\"}\n",
    "    pload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": 0.5,\n",
    "        \"stop\": SEP,\n",
    "        \"images\": images,\n",
    "    }\n",
    "\n",
    "    if LLAVA_MODE == \"local\":\n",
    "        response = requests.post(WORKER_ADDR + \"/worker_generate_stream\", headers=headers,\n",
    "                json=pload, stream=False)\n",
    "\n",
    "        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n",
    "            if chunk:\n",
    "                data = json.loads(chunk.decode(\"utf-8\"))\n",
    "                output = data[\"text\"].split(SEP)[-1]\n",
    "    elif LLAVA_MODE == \"remote\":\n",
    "        # The Replicate version of the model only support 1 image for now.\n",
    "        img = 'data:image/jpeg;base64,' + images[0]\n",
    "        response = replicate.run(\n",
    "            \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "            input={\"image\": img, \"prompt\": prompt.replace(\"<image>\", \" \")}\n",
    "        )\n",
    "        # The yorickvp/llava-13b model can stream output as it's running.\n",
    "        # The predict method returns an iterator, and you can iterate over that output.\n",
    "        output = \"\"\n",
    "        for item in response:\n",
    "            # https://replicate.com/yorickvp/llava-13b/versions/2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591/api#output-schema\n",
    "            output += item\n",
    "        \n",
    "    # Remove the prompt and the space.\n",
    "    output = output.replace(prompt, \"\").strip().rstrip()\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123df2c",
   "metadata": {},
   "source": [
    "Here is the image that we are going to use.\n",
    "\n",
    "![Image](https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec31ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a small, red, and black toy with glasses. The toy appears to be a stuffed animal or a figurine, possibly a cute red lizard or a dragon. The red color and the flames on the toy give it a fiery appearance. The toy is placed on a grey surface, which might be a table or a shelf. The glasses add a playful and quirky touch to the overall design, making the toy appear more charming and unique.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image: <image>\", \n",
    "                 images=[\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a7db5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statue is a small animal, possibly a horse, with a fire element in its design. It has flames on its back and is wearing glasses. The fire element adds a unique and interesting visual effect to the statue, making it an eye-catching piece of art.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image in one sentence: https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4faf59",
   "metadata": {},
   "source": [
    "## Application 1: Garden Helper\n",
    "\n",
    "\n",
    "Here we demonstrate a very simple multi-agent collaboration on creating visualization.\n",
    "\n",
    "The user will upload an image of their garden, the image agent (with LLaVA backend) will read the image and describe the problem. Then, the suggestion agent (AssistantAgent with GPT model) will give suggestions on how to treat the problem.\n",
    "\n",
    "\n",
    "Here, we found a problem in our garden and took a photo:\n",
    "![](http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286938aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, Agent\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67157629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "What's wrong with my strawberries? http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage-explainer\u001b[0m (to chat_manager):\n",
      "\n",
      "I am a garden helper and I have been asked to look at this photo of strawberries. The plants appear to be in poor health, with wilted leaves and small, unripe berries. There are also some pests visible on the plants, which could be contributing to their poor health. It is possible that the plants are not receiving enough water, or there may be issues with the soil quality. Additionally, the presence of pests could indicate that the plants are not being adequately maintained. To improve the health of these plants, it would be beneficial to address these issues by providing proper care, such as ensuring that the plants receive sufficient water and nutrients, and implementing measures to control pests.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSuggestion-Giver\u001b[0m (to chat_manager):\n",
      "\n",
      "To improve your strawberry plants' health:\n",
      "\n",
      "1. Water consistently, avoiding overwatering or underwatering.\n",
      "2. Apply organic compost or balanced fertilizer to improve soil quality.\n",
      "3. Remove visible pests and consider using a natural pesticide to prevent further infestations.\n",
      "4. Maintain your garden by pruning, weeding, and ensuring proper spacing between plants.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ImageAgent(AssistantAgent):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.register_reply([Agent, None], reply_func=ImageAgent._image_reply)\n",
    "        \n",
    "    def _image_reply(\n",
    "        self,\n",
    "        messages=None,\n",
    "        sender=None, config=None\n",
    "    ):\n",
    "        # Note: we did not use \"llm_config\" yet.\n",
    "        # TODO: make the LLaVA design compatible with llm_config\n",
    "        if all((messages is None, sender is None)):\n",
    "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
    "            logger.error(error_msg)\n",
    "            raise AssertionError(error_msg)\n",
    "\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "\n",
    "        msg = messages[-1][\"content\"]\n",
    "#         prompt = \"For the image: <image>\\n\\n\" + self.system_message\n",
    "\n",
    "        prompt = f\"[INST]{self.system_message}[/INST]\" + msg\n",
    "        \n",
    "        out = \"\"\n",
    "        retry = 5\n",
    "        while len(out) == 0 and retry > 0:\n",
    "            # image names will be inferred automatically from llava_call\n",
    "            out = llava_call(prompt=prompt)\n",
    "            retry -= 1\n",
    "            \n",
    "        assert out != \"\", \"Empty response from LLaVA.\"\n",
    "        \n",
    "        \n",
    "        return True, out\n",
    "\n",
    "\n",
    "image_agent = ImageAgent(\n",
    "    name=\"image-explainer\",\n",
    "    system_message=\"You are a garden helper! You need to describe what are in the image, and highlight the problems with the plants and the garden\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\"\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "suggestion_giver = autogen.AssistantAgent(\n",
    "    name=\n",
    "    \"Suggestion-Giver\",\n",
    "    system_message=\"Give me treatment suggestions for my garden! You can find the description of my image from the image-explainer agent. Keep the answer concise and short.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, image_agent, suggestion_giver],\n",
    "                              messages=[],\n",
    "                              max_round=3)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "\n",
    "# Ask the question with an image\n",
    "user_proxy.initiate_chat(manager, \n",
    "                         message=\"What's wrong with my strawberries? http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefdcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59a919e1",
   "metadata": {},
   "source": [
    "# Application 2: Image Regeneration\n",
    "\n",
    "\n",
    "> Ackowledgement: We draw inspirations from the [GitHub project](https://github.com/JayZeeDesign/vision-agent-with-llava/blob/main/app.py) and [YouTube video](https://www.youtube.com/watch?v=JgVb8A6OJwM&t=1s&ab_channel=AIJason) from [AI Jason](https://www.ai-jason.com/)\n",
    "\n",
    "Input an image, and two agents collaborate together:\n",
    "- Generator: an agent that generate better prompts to plot figure, and invoke DALLE to generate images. \n",
    "- Critic: a discriminator to tell why the two images are different\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
