{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c75da30",
   "metadata": {},
   "source": [
    "# Agent Chat with Multi-Modality Models\n",
    "\n",
    "Here, we use LLaVA as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805e4bd",
   "metadata": {},
   "source": [
    "## LLaVA Setup\n",
    "Please follow the LLaVA Github [page](https://github.com/haotian-liu/LLaVA/) to install the LLaVA, download weights, and start the server.\n",
    "\n",
    "For instance, here are some important steps:\n",
    "```bash\n",
    "# Download package\n",
    "git clone https://github.com/haotian-liu/LLaVA.git\n",
    "cd LLaVA\n",
    "\n",
    "# Install inference package\n",
    "conda create -n llava python=3.10 -y\n",
    "conda activate llava\n",
    "pip install --upgrade pip  # enable PEP 660 support\n",
    "pip install -e .\n",
    "\n",
    "# Download and serve the model\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\n",
    "```\n",
    "\n",
    "Some helpful packages and dependencies:\n",
    "```bash\n",
    "conda install -c nvidia cuda-toolkit\n",
    "```\n",
    "\n",
    "\n",
    "### Launch\n",
    "\n",
    "In one terminal, start the controller\n",
    "```bash\n",
    "python -m llava.serve.controller --host 0.0.0.0 --port 10000\n",
    "```\n",
    "\n",
    "\n",
    "In another terminal, start the worker, which will load the model to GPU\n",
    "```bash\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29925f",
   "metadata": {},
   "source": [
    "**Note: make sure the environment of this notebook also installed the llava package from `pip install -e .`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bf7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-13 17:31:15,781] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from llava.conversation import default_conversation as conv\n",
    "from llava.conversation import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48a9bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: llava-v1.5-13b\n"
     ]
    }
   ],
   "source": [
    "# Setup some global constants for convenience\n",
    "WORKER_ADDR = \"http://0.0.0.0:40000\"\n",
    "CONTROLLER_ADDR = \"http://0.0.0.0:10000\"\n",
    "SEP =  conv.sep\n",
    "ret = requests.post(CONTROLLER_ADDR + \"/list_models\")\n",
    "MODEL_NAME = ret.json()[\"models\"][0]\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430de743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf7f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_data(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        content = response.content\n",
    "    elif image_file.startswith(\"data:image/png;base64,\"):\n",
    "        return image_file.replace(\"data:image/png;base64,\", \"\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        content = open(image_file, \"rb\").read()\n",
    "    return base64.b64encode(content).decode('utf-8')\n",
    "    \n",
    "def _to_pil(data):\n",
    "    return Image.open(BytesIO(base64.b64decode(data)))\n",
    "\n",
    "\n",
    "def llava_call(prompt:str, model_name: str=MODEL_NAME, images: list=[], max_new_tokens:int=1000) -> str:\n",
    "    assert prompt.count(\"<image>\") == len(images), \"the number \"\n",
    "    \"of image token in prompt and in the images list should be the same!\"\n",
    "    \n",
    "    images = [get_image_data(x) for x in images]\n",
    "    \n",
    "    for im in images:\n",
    "        if len(im) == 0:\n",
    "            raise RunTimeError(\"An image is empty!\")\n",
    "            \n",
    "    headers = {\"User-Agent\": \"LLaVA Client\"}\n",
    "    pload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": 0.5,\n",
    "        \"stop\": SEP,\n",
    "        \"images\": images,\n",
    "    }\n",
    "\n",
    "    response = requests.post(WORKER_ADDR + \"/worker_generate_stream\", headers=headers,\n",
    "            json=pload, stream=False)\n",
    "\n",
    "    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data = json.loads(chunk.decode(\"utf-8\"))\n",
    "            output = data[\"text\"].split(SEP)[-1]\n",
    "    \n",
    "    # Remove the prompt and the space.\n",
    "    output = output.replace(prompt, \"\").strip().rstrip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123df2c",
   "metadata": {},
   "source": [
    "Here is the image that we are going to use.\n",
    "\n",
    "![Image](https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec31ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, a small toy is on display. The toy is an orange, flame-covered animal, resembling a lizard or a small horse. It is sitting on a table, and its fur is set on fire. The toy has a pair of red glasses on its face, which adds to its unique appearance. The overall scene is quite captivating and fun, with the toy's flames giving it a distinctive and eye-catching look.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image: <image>\", \n",
    "                 images=[\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4faf59",
   "metadata": {},
   "source": [
    "## AutoGen Integration: Garden Helper\n",
    "\n",
    "\n",
    "Here we demonstrate a very simple multi-agent collaboration on creating visualization.\n",
    "\n",
    "The user will upload an image of their garden, the image agent (with LLaVA backend) will read the image and describe the problem. Then, the suggestion agent (AssistantAgent with GPT model) will give suggestions on how to treat the problem.\n",
    "\n",
    "\n",
    "Here, we found a problem in our garden and took a photo:\n",
    "![](http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "286938aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67157629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ImageAgent(AssistantAgent):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.register_reply([Agent, None], reply_func=ImageAgent._image_reply)\n",
    "        \n",
    "    def _image_reply(\n",
    "        self,\n",
    "        messages=None,\n",
    "        sender=None, config=None\n",
    "    ):\n",
    "        # Note: we did not use \"llm_config\" yet.\n",
    "        # TODO: make the LLaVA design compatible with llm_config\n",
    "        if all((messages is None, sender is None)):\n",
    "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
    "            logger.error(error_msg)\n",
    "            raise AssertionError(error_msg)\n",
    "\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "\n",
    "        image_name = messages[-1][\"content\"]\n",
    "        prompt = \"For the image: <image>\\n\\n\" + self.system_message\n",
    "        out = llava_call(prompt=prompt,\n",
    "                         images=[image_name, ])\n",
    "\n",
    "        print(out)\n",
    "        assert out != \"\", \"Empty response from LLaVA.\"\n",
    "        \n",
    "        \n",
    "        return True, out\n",
    "\n",
    "\n",
    "image_agent = ImageAgent(\n",
    "    name=\"image-explainer\",\n",
    "    system_message=\"What is in the image?\\nHighlight the problems with the plants!\\nDescribe in as many details as possible.\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\"\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "suggestion_giver = autogen.AssistantAgent(\n",
    "    name=\n",
    "    \"Suggestion-Giver\",\n",
    "    system_message=\"Give me treatment suggestions for my garden! You can find the description of my image from the image-explainer agent. Keep the answer concise and short.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, image_agent, suggestion_giver],\n",
    "                              messages=[],\n",
    "                              max_round=3)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "\n",
    "# Ask the question with an image\n",
    "user_proxy.initiate_chat(manager, \n",
    "                         message=\"http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f0fe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06cd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefdcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75f229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
