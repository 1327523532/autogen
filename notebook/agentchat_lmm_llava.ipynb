{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c75da30",
   "metadata": {},
   "source": [
    "# Agent Chat with Multi-Modality Models\n",
    "\n",
    "Here, we use LLaVA as an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805e4bd",
   "metadata": {},
   "source": [
    "## LLaVA Setup\n",
    "Please follow the LLaVA GitHub [page](https://github.com/haotian-liu/LLaVA/) to install LLaVA, download the weights, and start the server.\n",
    "\n",
    "For instance, here are some important steps:\n",
    "```bash\n",
    "# Download the package\n",
    "git clone https://github.com/haotian-liu/LLaVA.git\n",
    "cd LLaVA\n",
    "\n",
    "# Install the inference package\n",
    "conda create -n llava python=3.10 -y\n",
    "conda activate llava\n",
    "pip install --upgrade pip  # enable PEP 660 support\n",
    "pip install -e .\n",
    "\n",
    "# Download and serve the model\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\n",
    "```\n",
    "\n",
    "Some helpful packages and dependencies:\n",
    "```bash\n",
    "conda install -c nvidia cuda-toolkit\n",
    "```\n",
    "\n",
    "\n",
    "### Launch\n",
    "\n",
    "In one terminal, start the controller first:\n",
    "```bash\n",
    "python -m llava.serve.controller --host 0.0.0.0 --port 10000\n",
    "```\n",
    "\n",
    "\n",
    "Then, in another terminal, start the worker, which will load the model to the GPU:\n",
    "```bash\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29925f",
   "metadata": {},
   "source": [
    "**Note: make sure the environment of this notebook also installed the llava package from `pip install -e .`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93bf7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 10:06:11,529] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from llava.conversation import default_conversation as conv\n",
    "from llava.conversation import Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f48a9bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': ['llava-v1.5-13b']}\n",
      "Model Name: llava-v1.5-13b\n"
     ]
    }
   ],
   "source": [
    "# Setup some global constants for convenience\n",
    "WORKER_ADDR = \"http://0.0.0.0:40000\"\n",
    "CONTROLLER_ADDR = \"http://0.0.0.0:10000\"\n",
    "SEP =  conv.sep\n",
    "ret = requests.post(CONTROLLER_ADDR + \"/list_models\")\n",
    "print(ret.json())\n",
    "MODEL_NAME = ret.json()[\"models\"][0]\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313bc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf7f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def extract_img_paths(paragraph: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract image paths (URLs or local paths) from a text paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The input text paragraph.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted image paths.\n",
    "    \"\"\"\n",
    "    # Regular expression to match image URLs and file paths\n",
    "    img_path_pattern = re.compile(r'\\b(?:http[s]?://\\S+\\.(?:jpg|jpeg|png|gif|bmp)|\\S+\\.(?:jpg|jpeg|png|gif|bmp))\\b', \n",
    "                                  re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the paragraph\n",
    "    img_paths = re.findall(img_path_pattern, paragraph)\n",
    "    return img_paths\n",
    "\n",
    "\n",
    "def get_image_data(image_file):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        content = response.content\n",
    "    elif image_file.startswith(\"data:image/png;base64,\"):\n",
    "        return image_file.replace(\"data:image/png;base64,\", \"\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        content = open(image_file, \"rb\").read()\n",
    "    return base64.b64encode(content).decode('utf-8')\n",
    "    \n",
    "def _to_pil(data):\n",
    "    return Image.open(BytesIO(base64.b64decode(data)))\n",
    "\n",
    "\n",
    "def llava_call(prompt:str, model_name: str=MODEL_NAME, images: list=[], max_new_tokens:int=1000) -> str:\n",
    "    \"\"\"\n",
    "    Makes a call to the LLaVA service to generate text based on a given prompt and optionally provided images.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): The input text for the model. Any image paths or placeholders in the text should be replaced with \"<image>\".\n",
    "        - model_name (str, optional): The name of the model to use for the text generation. Defaults to the global constant MODEL_NAME.\n",
    "        - images (list, optional): A list of image paths or URLs. If not provided, they will be extracted from the prompt.\n",
    "            If provided, they will be appended to the prompt with the \"<image>\" placeholder.\n",
    "        - max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        - str: Generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        - AssertionError: If the number of \"<image>\" tokens in the prompt and the number of provided images do not match.\n",
    "        - RunTimeError: If any of the provided images is empty.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses global constants: WORKER_ADDR and SEP.\n",
    "    - Any image paths or URLs in the prompt are automatically replaced with the \"<image>\" token.\n",
    "    - If more images are provided than there are \"<image>\" tokens in the prompt, the extra tokens are appended to the end of the prompt.\n",
    "    \"\"\"\n",
    "    if len(images) == 0:\n",
    "        images = extract_img_paths(prompt)\n",
    "        for im in images:\n",
    "            prompt = prompt.replace(im, \"<image>\")\n",
    "    else:\n",
    "        # Append the <image> token if missing\n",
    "        assert prompt.count(\"<image>\") <= len(images), \"the number \"\n",
    "        \"of image token in prompt and in the images list should be the same!\"\n",
    "        num_token_missing = len(images) - prompt.count(\"<image>\")\n",
    "        prompt += \" <image> \" * num_token_missing\n",
    "\n",
    "    \n",
    "    images = [get_image_data(x) for x in images]\n",
    "    \n",
    "    for im in images:\n",
    "        if len(im) == 0:\n",
    "            raise RunTimeError(\"An image is empty!\")\n",
    "            \n",
    "    headers = {\"User-Agent\": \"LLaVA Client\"}\n",
    "    pload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": 0.5,\n",
    "        \"stop\": SEP,\n",
    "        \"images\": images,\n",
    "    }\n",
    "\n",
    "    response = requests.post(WORKER_ADDR + \"/worker_generate_stream\", headers=headers,\n",
    "            json=pload, stream=False)\n",
    "\n",
    "    for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data = json.loads(chunk.decode(\"utf-8\"))\n",
    "            output = data[\"text\"].split(SEP)[-1]\n",
    "    \n",
    "    # Remove the prompt and the space.\n",
    "    output = output.replace(prompt, \"\").strip().rstrip()\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123df2c",
   "metadata": {},
   "source": [
    "Here is the image that we are going to use.\n",
    "\n",
    "![Image](https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec31ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a figurine of a red, fire-breathing, spiky-haired animal, possibly a lizard or a dragon. The figurine is made of plastic and has some orange flames coming from the top of its head. The figurine is wearing glasses and has a fire-breathing effect added to it.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image: <image>\", \n",
    "                 images=[\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee3069a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a small red toy animal that is shaped like a llama. The toy is wearing glasses and has flames on its body. The toy is standing on a grey surface, possibly a table.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image in one sentence: https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868cbddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A math equation with variables\n",
      "\n",
      "This image shows a math equation with variables and numbers in a black and white format. The equation appears to be a combination of logarithmic and trigonometric functions, with variables such as x, y, and z. The equation is written on a white background, which emphasizes the mathematical symbols and numbers. The presence of the variables and the complex function suggests that the equation might be used in scientific or technical applications.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Here is a latex formular. Can you type it out for me? <image>\", \n",
    "                 images=[\"https://th.bing.com/th/id/OIP.koxFBp0VFEzqeiNL9diaUwHaBY?pid=ImgDet&rs=1\"])\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4faf59",
   "metadata": {},
   "source": [
    "## AutoGen Integration: Garden Helper\n",
    "\n",
    "\n",
    "Here we demonstrate a very simple multi-agent collaboration on creating visualization.\n",
    "\n",
    "The user will upload an image of their garden, the image agent (with LLaVA backend) will read the image and describe the problem. Then, the suggestion agent (AssistantAgent with GPT model) will give suggestions on how to treat the problem.\n",
    "\n",
    "\n",
    "Here, we found a problem in our garden and took a photo:\n",
    "![](http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286938aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import autogen\n",
    "from autogen import AssistantAgent, Agent\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67157629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage-explainer\u001b[0m (to chat_manager):\n",
      "\n",
      "In the image, there is a bunch of strawberries on a tarp. The strawberries are fresh and ripe, with some of them appearing to be overripe. They are surrounded by several green leaves, which are likely part of the strawberry plant. The tarp appears to be covering the strawberries and leaves, possibly for protection or to keep them organized for transportation or sale.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mSuggestion-Giver\u001b[0m (to chat_manager):\n",
      "\n",
      "1. Regularly water your plants, especially during dry seasons, but avoid over-watering.\n",
      "2. Use organic mulch to regulate soil temperature, retain moisture, and prevent weed growth.\n",
      "3. Prune old leaves and remove overripe fruit to encourage new growth and prevent diseases.\n",
      "4. Use netting or a tarp to protect your plants from birds and insects.\n",
      "5. Apply a balanced, organic fertilizer during the growing season to support fruit production.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class ImageAgent(AssistantAgent):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.register_reply([Agent, None], reply_func=ImageAgent._image_reply)\n",
    "        \n",
    "    def _image_reply(\n",
    "        self,\n",
    "        messages=None,\n",
    "        sender=None, config=None\n",
    "    ):\n",
    "        # Note: we did not use \"llm_config\" yet.\n",
    "        # TODO: make the LLaVA design compatible with llm_config\n",
    "        if all((messages is None, sender is None)):\n",
    "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
    "            logger.error(error_msg)\n",
    "            raise AssertionError(error_msg)\n",
    "\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "\n",
    "        image_name = messages[-1][\"content\"]\n",
    "        prompt = \"For the image: <image>\\n\\n\" + self.system_message\n",
    "        \n",
    "        out = \"\"\n",
    "        retry = 5\n",
    "        while len(out) == 0 and retry > 0:\n",
    "            out = llava_call(prompt=prompt,\n",
    "                             images=[image_name, ])\n",
    "            retry -= 1\n",
    "            \n",
    "        assert out != \"\", \"Empty response from LLaVA.\"\n",
    "        \n",
    "        \n",
    "        return True, out\n",
    "\n",
    "\n",
    "image_agent = ImageAgent(\n",
    "    name=\"image-explainer\",\n",
    "    system_message=\"What is in the image?\\nHighlight the problems with the plants!\\nDescribe in as many details as possible.\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\"\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "suggestion_giver = autogen.AssistantAgent(\n",
    "    name=\n",
    "    \"Suggestion-Giver\",\n",
    "    system_message=\"Give me treatment suggestions for my garden! You can find the description of my image from the image-explainer agent. Keep the answer concise and short.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, image_agent, suggestion_giver],\n",
    "                              messages=[],\n",
    "                              max_round=3)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "\n",
    "# Ask the question with an image\n",
    "user_proxy.initiate_chat(manager, \n",
    "                         message=\"http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f0fe5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06cd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fefdcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75f229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
