{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c75da30",
   "metadata": {},
   "source": [
    "# Agent Chat with Multi-Modality Models\n",
    "\n",
    "We use **LLaVA** as an example for the multi-modality feature.  More information about LLaVA can be found in their [GitHub page](https://github.com/haotian-liu/LLaVA)\n",
    "\n",
    "\n",
    "This notebook contains the following information and examples:\n",
    "\n",
    "1. Setup LLaVA \n",
    "    - Option 1: Use API calls from `Replicate`\n",
    "    - Option 2: Setup LLaVA locally (requires GPU)\n",
    "2. Application 1: Garden helper\n",
    "3. Application 2: Image Regeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ffe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use this variable to control where you want to host LLaVA, locally or remotely?\n",
    "# More details in the two setup options below.\n",
    "\n",
    "LLAVA_MODE = \"local\" # Either \"local\" or \"remote\"\n",
    "\n",
    "assert LLAVA_MODE in [\"local\", \"remote\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4703b",
   "metadata": {},
   "source": [
    "# (Option 1, preferred) Use API Calls from Replicate [Remote]\n",
    "We can also use [Replicate](https://replicate.com/yorickvp/llava-13b/api) to use LLaVA directly, which will host the model for you.\n",
    "\n",
    "1. Run `pip install replicate` to install the package\n",
    "2. You need to get an API key from Replicate from your [account setting page](https://replicate.com/account/api-tokens)\n",
    "3. Next, copy your API token and authenticate by setting it as an environment variable:\n",
    "    `export REPLICATE_API_TOKEN=<paste-your-token-here>` \n",
    "4. You need to enter your credit card information for Replicate ðŸ¥²\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f650bf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install replicate\n",
    "# import os\n",
    "## alternatively, you can put your API key here for the environment variable.\n",
    "# os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_xyz your api key goes here~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267ffd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e313bc00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# img = get_image_data(\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\", use_b64=True)\n",
    "# img = 'data:image/jpeg;base64,' + img\n",
    "\n",
    "# response = replicate.run(\n",
    "#     \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "#     input={\"image\": img,\n",
    "#            \"prompt\": \"Describe the in poetry.\"}\n",
    "# )\n",
    "# output = \"\"\n",
    "# for item in response:\n",
    "#     output += item\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805e4bd",
   "metadata": {},
   "source": [
    "## [Option 2] Setup LLaVA Locally\n",
    "Please follow the LLaVA GitHub [page](https://github.com/haotian-liu/LLaVA/) to install LLaVA, download the weights, and start the server.\n",
    "\n",
    "For instance, here are some important steps:\n",
    "```bash\n",
    "# Download the package\n",
    "git clone https://github.com/haotian-liu/LLaVA.git\n",
    "cd LLaVA\n",
    "\n",
    "# Install the inference package\n",
    "conda create -n llava python=3.10 -y\n",
    "conda activate llava\n",
    "pip install --upgrade pip  # enable PEP 660 support\n",
    "pip install -e .\n",
    "\n",
    "# Download and serve the model\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-7b\n",
    "```\n",
    "\n",
    "Some helpful packages and dependencies:\n",
    "```bash\n",
    "conda install -c nvidia cuda-toolkit\n",
    "```\n",
    "\n",
    "\n",
    "### Launch\n",
    "\n",
    "In one terminal, start the controller first:\n",
    "```bash\n",
    "python -m llava.serve.controller --host 0.0.0.0 --port 10000\n",
    "```\n",
    "\n",
    "\n",
    "Then, in another terminal, start the worker, which will load the model to the GPU:\n",
    "```bash\n",
    "python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path liuhaotian/llava-v1.5-13b\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29925f",
   "metadata": {},
   "source": [
    "**Note: make sure the environment of this notebook also installed the llava package from `pip install -e .`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93bf7915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-18 00:09:02,768] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "{'models': ['llava-v1.5-13b']}\n",
      "Model Name: llava-v1.5-13b\n"
     ]
    }
   ],
   "source": [
    "# Run this code block only if you want to run LlaVA locally\n",
    "import requests\n",
    "import json\n",
    "from llava.conversation import default_conversation as conv\n",
    "from llava.conversation import Conversation\n",
    "\n",
    "# Setup some global constants for convenience\n",
    "# Note: make sure the addresses below are consistent with your setup in LLaVA \n",
    "CONTROLLER_ADDR = \"http://0.0.0.0:10000\"\n",
    "SEP =  conv.sep\n",
    "ret = requests.post(CONTROLLER_ADDR + \"/list_models\")\n",
    "print(ret.json())\n",
    "MODEL_NAME = ret.json()[\"models\"][0]\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307852dd",
   "metadata": {},
   "source": [
    "# Multi-Modal Functions\n",
    "\n",
    "The Multi-Modal Functions library provides a set of utilities to manage and process multi-modal data, focusing on textual and image components. The library allows you to format prompts, extract image paths, and handle image data in various formats.\n",
    "\n",
    "## Functions\n",
    "\n",
    "\n",
    "### `get_image_data`\n",
    "\n",
    "This function retrieves the content of an image specified by a file path or URL and optionally converts it to base64 format. It can handle both web-hosted images and locally stored files.\n",
    "\n",
    "\n",
    "### `lmm_formater`\n",
    "\n",
    "This function formats a user-provided prompt containing `<img ...>` tags, replacing these tags with `<image>` or numbered versions like `<image 1>`, `<image 2>`, etc., and extracts the image locations. It returns a tuple containing the new formatted prompt and a list of image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7986fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf7f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def get_image_data(image_file, use_b64=True):\n",
    "    if image_file.startswith('http://') or image_file.startswith('https://'):\n",
    "        response = requests.get(image_file)\n",
    "        content = response.content\n",
    "    elif re.match(r\"data:image/(?:png|jpeg);base64,\", image_file):\n",
    "        return re.sub(r\"data:image/(?:png|jpeg);base64,\", \"\", image_file)\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "#         content = open(image_file, \"rb\").read()\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"PNG\")\n",
    "        content = buffered.getvalue()\n",
    "        \n",
    "    if use_b64:\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "\n",
    "def lmm_formater(prompt: str, order_image_tokens: bool = False) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Formats the input prompt by replacing image tags and returns the new prompt along with image locations.\n",
    "    \n",
    "    Parameters:\n",
    "        - prompt (str): The input string that may contain image tags like <img ...>.\n",
    "        - order_image_tokens (bool, optional): Whether to order the image tokens with numbers. \n",
    "            It will be useful for GPT-4V. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        - Tuple[str, List[str]]: A tuple containing the formatted string and a list of images (loaded in b64 format).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    new_prompt = prompt\n",
    "    image_locations = []\n",
    "    images = []\n",
    "    image_count = 0\n",
    "    \n",
    "    # Regular expression pattern for matching <img ...> tags\n",
    "    img_tag_pattern = re.compile(r'<img ([^>]+)>')\n",
    "    \n",
    "    # Find all image tags\n",
    "    for match in img_tag_pattern.finditer(prompt):\n",
    "        image_location = match.group(1)\n",
    "        \n",
    "        try: \n",
    "            img_data = get_image_data(image_location)\n",
    "        except:\n",
    "            # Remove the token\n",
    "            print(f\"Warning! Unable to load image from {image_location}\")\n",
    "            new_prompt = new_prompt.replace(match.group(0), \"\", 1)\n",
    "            continue\n",
    "        \n",
    "        image_locations.append(image_location)\n",
    "        images.append(img_data)\n",
    "        \n",
    "        # Increment the image count and replace the tag in the prompt\n",
    "        new_token = f'<image {image_count}>' if  order_image_tokens else \"<image>\"\n",
    "\n",
    "        new_prompt = new_prompt.replace(match.group(0), new_token, 1)\n",
    "        image_count += 1\n",
    "        \n",
    "    return new_prompt, images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_img_paths(paragraph: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract image paths (URLs or local paths) from a text paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        paragraph (str): The input text paragraph.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of extracted image paths.\n",
    "    \"\"\"\n",
    "    # Regular expression to match image URLs and file paths\n",
    "    img_path_pattern = re.compile(r'\\b(?:http[s]?://\\S+\\.(?:jpg|jpeg|png|gif|bmp)|\\S+\\.(?:jpg|jpeg|png|gif|bmp))\\b', \n",
    "                                  re.IGNORECASE)\n",
    "    \n",
    "    # Find all matches in the paragraph\n",
    "    img_paths = re.findall(img_path_pattern, paragraph)\n",
    "    return img_paths\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def _to_pil(data):\n",
    "    return Image.open(BytesIO(base64.b64decode(data)))\n",
    "\n",
    "\n",
    "def llava_call(prompt:str, model_name: str=MODEL_NAME, images: list=[], max_new_tokens:int=1000, temperature: float=0.5) -> str:\n",
    "    \"\"\"\n",
    "    Makes a call to the LLaVA service to generate text based on a given prompt and optionally provided images.\n",
    "\n",
    "    Args:\n",
    "        - prompt (str): The input text for the model. Any image paths or placeholders in the text should be replaced with \"<image>\".\n",
    "        - model_name (str, optional): The name of the model to use for the text generation. Defaults to the global constant MODEL_NAME.\n",
    "        - images (list, optional): A list of image paths or URLs. If not provided, they will be extracted from the prompt.\n",
    "            If provided, they will be appended to the prompt with the \"<image>\" placeholder.\n",
    "        - max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 1000.\n",
    "        - temperature (float, optional): temperature for the model. Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        - str: Generated text from the model.\n",
    "\n",
    "    Raises:\n",
    "        - AssertionError: If the number of \"<image>\" tokens in the prompt and the number of provided images do not match.\n",
    "        - RunTimeError: If any of the provided images is empty.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses global constants: CONTROLLER_ADDR and SEP.\n",
    "    - Any image paths or URLs in the prompt are automatically replaced with the \"<image>\" token.\n",
    "    - If more images are provided than there are \"<image>\" tokens in the prompt, the extra tokens are appended to the end of the prompt.\n",
    "    \"\"\"\n",
    "    if len(images) == 0:\n",
    "        prompt, images = lmm_formater(prompt, order_image_tokens=False)\n",
    "    else:\n",
    "        # Append the <image> token if missing\n",
    "        assert prompt.count(\"<image>\") <= len(images), \"the number \"\n",
    "        \"of image token in prompt and in the images list should be the same!\"\n",
    "        num_token_missing = len(images) - prompt.count(\"<image>\")\n",
    "        prompt += \" <image> \" * num_token_missing\n",
    "        images = [get_image_data(x) for x in images]\n",
    "    \n",
    "    for im in images:\n",
    "        if len(im) == 0:\n",
    "            raise RunTimeError(\"An image is empty!\")\n",
    "\n",
    "    if LLAVA_MODE == \"local\":\n",
    "        headers = {\"User-Agent\": \"LLaVA Client\"}\n",
    "        pload = {\n",
    "            \"model\": model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"stop\": SEP,\n",
    "            \"images\": images,\n",
    "        }\n",
    "\n",
    "        response = requests.post(CONTROLLER_ADDR + \"/worker_generate_stream\", headers=headers,\n",
    "                json=pload, stream=False)\n",
    "\n",
    "        for chunk in response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b\"\\0\"):\n",
    "            if chunk:\n",
    "                data = json.loads(chunk.decode(\"utf-8\"))\n",
    "                output = data[\"text\"].split(SEP)[-1]\n",
    "    elif LLAVA_MODE == \"remote\":\n",
    "        # The Replicate version of the model only support 1 image for now.\n",
    "        img = 'data:image/jpeg;base64,' + images[0]\n",
    "        response = replicate.run(\n",
    "            \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n",
    "            input={\"image\": img, \"prompt\": prompt.replace(\"<image>\", \" \")}\n",
    "        )\n",
    "        # The yorickvp/llava-13b model can stream output as it's running.\n",
    "        # The predict method returns an iterator, and you can iterate over that output.\n",
    "        output = \"\"\n",
    "        for item in response:\n",
    "            # https://replicate.com/yorickvp/llava-13b/versions/2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591/api#output-schema\n",
    "            output += item\n",
    "        \n",
    "    # Remove the prompt and the space.\n",
    "    output = output.replace(prompt, \"\").strip().rstrip()\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123df2c",
   "metadata": {},
   "source": [
    "Here is the image that we are going to use.\n",
    "\n",
    "![Image](https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed5a35",
   "metadata": {},
   "source": [
    "We can call llava by providing the prompt and images separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec31ca74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image, a red and black stuffed animal is on display. The stuffed animal is wearing glasses and has flames coming out of it. The stuffed animal is sitting on a surface, possibly a table or a shelf. The glasses on the stuffed animal add a unique touch to the overall design, making it a playful and interesting decoration.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image: <image>\", \n",
    "                 images=[\"https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png\"])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619dc30",
   "metadata": {},
   "source": [
    "Or, we can also call LLaVA with only prompt, with images embedded in the prompt with the <img xxx> format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a7db5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humorous and unusual, the toy is designed to resemble a fire-breathing lizard or a red, flame-covered creature. The toy has a face with big eyes and wears glasses, giving it a quirky, amusing appearance. The toy is sitting on a gray background, making the vibrant colors of the fire and the red plastic stand out. This toy is likely meant to be a fun and entertaining item for children or collectors who appreciate unique and whimsical plastic toys.\n"
     ]
    }
   ],
   "source": [
    "out = llava_call(\"Describe this image in one sentence: <img https://github.com/haotian-liu/LLaVA/raw/main/images/llava_logo.png>\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4faf59",
   "metadata": {},
   "source": [
    "## Application 1: Garden Helper\n",
    "\n",
    "In this section, we present a straightforward dual-agent architecture aimed at facilitating garden management.\n",
    "\n",
    "We identified an issue in our garden and captured it in the following photograph:\n",
    "![](http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0)\n",
    "\n",
    "Within the user proxy agent, we have activated the human input mode. This allows you to interact with LLaVA in a multi-round dialogue, enabling you to provide feedback as the conversation unfolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286938aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import AssistantAgent, Agent\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67157629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to image-explainer):\n",
      "\n",
      "Here is my image: <img http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0>. Can you give me some suggestions?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mimage-explainer\u001b[0m (to User_proxy):\n",
      "\n",
      "Human: Here is my image: <image>. Can you give me some suggestions?Human: Based on the image, I would suggest the following:\n",
      "\n",
      "1. Remove the dead leaves and any other debris from the strawberries and the surrounding area. This will help maintain the cleanliness and hygiene of the strawberries and prevent any potential contamination.\n",
      "2. Consider using a mesh net or a protective cover to shield the strawberries from pests and birds, ensuring that they grow in a safe and healthy environment.\n",
      "3. Regularly monitor the strawberries for any signs of pests, diseases, or other issues that might affect their growth or quality.\n",
      "4. Ensure that the strawberries receive adequate sunlight and water, as well as proper nutrients, to promote healthy growth and ripening.\n",
      "5. Keep the strawberries separated from other plants and crops to prevent cross-pollination or the spread of diseases.\n",
      "\n",
      "By following these suggestions, you can help maintain the health and quality of the strawberries and ensure a successful harvest.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Provide feedback to image-explainer. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class ImageAgent(AssistantAgent):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.register_reply([Agent, None], reply_func=ImageAgent._image_reply)\n",
    "        \n",
    "    def _image_reply(\n",
    "        self,\n",
    "        messages=None,\n",
    "        sender=None, config=None\n",
    "    ):\n",
    "        # Note: we did not use \"llm_config\" yet.\n",
    "        # TODO: make the LLaVA design compatible with llm_config\n",
    "        if all((messages is None, sender is None)):\n",
    "            error_msg = f\"Either {messages=} or {sender=} must be provided.\"\n",
    "            logger.error(error_msg)\n",
    "            raise AssertionError(error_msg)\n",
    "\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "\n",
    "        # The formats for LLaVA and GPT are different. So, we manually handle them here.\n",
    "        msg = messages[-1][\"content\"]\n",
    "        prompt = self.system_message + \"\\n\"\n",
    "        for msg in messages:\n",
    "            role = \"Human\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "            content = msg[\"content\"]\n",
    "            prompt += f\"{SEP}{role}: {content}\"\n",
    "        prompt += SEP\n",
    "#         print(prompt)\n",
    "    \n",
    "        out = \"\"\n",
    "        retry = 10\n",
    "        while len(out) == 0 and retry > 0:\n",
    "            # image names will be inferred automatically from llava_call\n",
    "            out = llava_call(prompt=prompt, temperature=random.random())\n",
    "            retry -= 1\n",
    "            \n",
    "        assert out != \"\", \"Empty response from LLaVA.\"\n",
    "        \n",
    "        \n",
    "        return True, out\n",
    "    \n",
    "    \n",
    "image_agent = ImageAgent(\n",
    "    name=\"image-explainer\",\n",
    "    system_message=\"You are a garden helper. You should describe the image and provide suggestions for the garden.\"\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"groupchat\"\n",
    "    },\n",
    "    human_input_mode=\"ALWAYS\", # Try between ALWAYS or NEVER\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# Ask the question with an image\n",
    "user_proxy.initiate_chat(image_agent, \n",
    "                         message=\"Here is my image: <img http://th.bing.com/th/id/R.105d684e5df7d540e61f6300d0bd374e?rik=PR8LCyvpe93DZA&pid=ImgRaw&r=0>. Can you give me some suggestions?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a919e1",
   "metadata": {},
   "source": [
    "# Application 2: Image Regeneration\n",
    "\n",
    "\n",
    "> Ackowledgement: We draw inspirations from the [GitHub project](https://github.com/JayZeeDesign/vision-agent-with-llava/blob/main/app.py) and [YouTube video](https://www.youtube.com/watch?v=JgVb8A6OJwM&t=1s&ab_channel=AIJason) from [AI Jason](https://www.ai-jason.com/)\n",
    "\n",
    "Input an image, and two agents collaborate together:\n",
    "- Generator: an agent that generate better prompts to plot figure, and invoke DALLE to generate images. \n",
    "- Critic: a discriminator to tell why the two images are different\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
